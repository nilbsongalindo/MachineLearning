{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 .How to build neural networks using nn.Module class\n",
    "## 2.How to build custom data input pipelines with data augmentation using Dataset and Dataloader classes.\n",
    "## 3.How to configure your learning rate with different learning rate schedules\n",
    "## 4.Training a Resnet bases image classifier to classify images from the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial from https://blog.paperspace.com/pytorch-101-building-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nn.Module class has two methods that we have to override: the __init__ (here we define various parameters of a layer such as filters, kernel size and etc) function and the foward function (doesn't need to be explicitly called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of use of foward\n",
    "\n",
    "class MyLayer(nn.Module):\n",
    "  def __init__(self, param):\n",
    "    super().__init__()\n",
    "    self.param = param \n",
    "  \n",
    "  def forward(self, x):\n",
    "    return x * self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25., 20., 15.])\n"
     ]
    }
   ],
   "source": [
    "myLayerObject = MyLayer(5)\n",
    "output = myLayerObject(torch.Tensor([5,4,3]))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another widely used and important class is the nn.Sequential class.  When initiating this class we can pass a list of nn.Module objects in a particular sequence. The object returned by nn.Sequential is itself a nn.Module object. When this object is run with an input, it sequentially runs the input through all the nn.Module object we passed to it, in the very same order as we passed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedNetwork = nn.Sequential(MyLayer(5), MyLayer(10))\n",
    "output = combinedNetwork([3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start the implementation of our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing the residual block\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # Conv Layer 1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels, out_channels=out_channels,\n",
    "            kernel_size=(3, 3), stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Conv Layer 2\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels, out_channels=out_channels,\n",
    "            kernel_size=(3, 3), stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "        # Shortcut connection to downsample residual\n",
    "        # In case the output dimensions of the residual block is not the same \n",
    "        # as it's input, have a convolutional layer downsample the layer \n",
    "        # being bought forward by approporate striding and filters\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels, out_channels=out_channels,\n",
    "                    kernel_size=(1, 1), stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = nn.ReLU()(out)\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # Initial input conv\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=64, kernel_size=(3, 3),\n",
    "            stride=1, padding=1, bias=False\n",
    "        )\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Create blocks\n",
    "        self.block1 = self._create_block(64, 64, stride=1)\n",
    "        self.block2 = self._create_block(64, 128, stride=2)\n",
    "        self.block3 = self._create_block(128, 256, stride=2)\n",
    "        self.block4 = self._create_block(256, 512, stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "    \n",
    "    # A block is just two residual blocks for ResNet18\n",
    "    def _create_block(self, in_channels, out_channels, stride):\n",
    "        return nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels, stride),\n",
    "            ResidualBlock(out_channels, out_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\t# Output of one layer becomes input to the next\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = nn.AvgPool2d(4)(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'cifar/train/'\n",
    "\n",
    "with open('cifar/labels.txt') as label_file:\n",
    "    labels = label_file.read().split()\n",
    "    label_mapping = dict(zip(labels, list(range(len(labels)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing will:\n",
    "\n",
    "1. Randomly horizontally the image with a probability of 0.5\n",
    "2. Normalise the image with mean and standard deviation of CIFAR dataset\n",
    "3. Reshape it from W  H  C to C  H  W.\n",
    "\"\"\"\n",
    "\n",
    "def preprocess(image):\n",
    "    image = np.array(image)\n",
    "    \n",
    "    if random.random() > 0.5:\n",
    "        image = image[::-1,:,:]\n",
    "    \n",
    "    cifar_mean = np.array([0.4914, 0.4822, 0.4465]).reshape(1,1,-1)\n",
    "    cifar_std  = np.array([0.2023, 0.1994, 0.2010]).reshape(1,1,-1)\n",
    "    image = (image - cifar_mean) / cifar_std\n",
    "    image = image.transpose(2,1,0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Normally, there are two classes PyTorch provides you in relation to build input pipelines to load data.\n",
    "\n",
    "1. torch.data.utils.dataset, which we will just refer as the dataset class now.\n",
    "\n",
    "2. torch.data.utils.dataloader , which we will just refer as the dataloader class now.\n",
    "\n",
    "## torch.utils.data.dataset\n",
    "dataset is a class that loads the data and returns a generator so that you iterate over it. It also lets you incorporate data augmentation techniques into the input Pipeline.\n",
    "\n",
    "If you want to create a dataset object for your data, you need to overload three functions.\n",
    "\n",
    "__init__ function. Here, you define things related to your dataset here. Most importantly, the location of your data. You can also define various data augmentations you want to apply.\n",
    "\n",
    "__len__ function. Here, you just return the length of the dataset.\n",
    "\n",
    "__getitem__ function. The function takes as an argument an index i and returns a data example. This function would be called every iteration during our training loop with a different i by the dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, data_size = 0, transforms = None):\n",
    "        files = os.listdir(data_dir)\n",
    "        files = [os.path.join(data_dir,x) for x in files]\n",
    "        \n",
    "        \n",
    "        if data_size < 0 or data_size > len(files):\n",
    "            assert(\"Data size should be between 0 to number of files in the dataset\")\n",
    "        \n",
    "        if data_size == 0:\n",
    "            data_size = len(files)\n",
    "        \n",
    "        self.data_size = data_size\n",
    "        self.files = random.sample(files, self.data_size)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_address = self.files[idx]\n",
    "        image = Image.open(image_address)\n",
    "        image = preprocess(image)\n",
    "        label_name = image_address[:-4].split(\"_\")[-1]\n",
    "        label = label_mapping[label_name]\n",
    "        \n",
    "        image = image.astype(np.float32)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.utils.data.Dataloader\n",
    "The Dataloader class facilitates\n",
    "\n",
    "1.Batching of Data\n",
    "\n",
    "2.Shuffling of Data\n",
    "\n",
    "3.Loading multiple data at a single time using threads\n",
    "\n",
    "4.Prefetching, that is, while GPU crunches the current batch, Dataloader can load the next batch into memory in meantime. This means GPU doesn't have to wait for the next batch and it speeds up training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Cifar10Dataset(data_dir='cifar/train/',transforms=None)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = Cifar10Dataset(data_dir='cifar/test/',transforms=None)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in trainloader: #or trainset\n",
    "    img, label = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "clf = ResNet()\n",
    "clf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(clf.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150,200],gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this tutorial is to show how Pytorch works, so we are not trying to obtain a high accurate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nilbson/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:118: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Index : 0 Loss : 2.450 Time : 1.612 seconds \n",
      "Batch Index : 100 Loss : 2.700 Time : 121.797 seconds \n",
      "Batch Index : 200 Loss : 2.334 Time : 120.991 seconds \n",
      "Batch Index : 300 Loss : 2.167 Time : 120.002 seconds \n",
      "Epoch : 0 Test Acc : 35.920\n",
      "--------------------------------------------------------------\n",
      "Batch Index : 0 Loss : 1.673 Time : 1.413 seconds \n",
      "Batch Index : 100 Loss : 1.615 Time : 121.027 seconds \n",
      "Batch Index : 200 Loss : 1.582 Time : 120.643 seconds \n",
      "Batch Index : 300 Loss : 1.544 Time : 121.539 seconds \n",
      "Epoch : 1 Test Acc : 49.710\n",
      "--------------------------------------------------------------\n",
      "Batch Index : 0 Loss : 1.373 Time : 1.596 seconds \n",
      "Batch Index : 100 Loss : 1.338 Time : 121.403 seconds \n",
      "Batch Index : 200 Loss : 1.308 Time : 122.117 seconds \n",
      "Batch Index : 300 Loss : 1.279 Time : 122.142 seconds \n",
      "Epoch : 2 Test Acc : 41.440\n",
      "--------------------------------------------------------------\n",
      "Batch Index : 0 Loss : 0.952 Time : 1.414 seconds \n",
      "Batch Index : 100 Loss : 1.095 Time : 121.870 seconds \n",
      "Batch Index : 200 Loss : 1.085 Time : 121.459 seconds \n",
      "Batch Index : 300 Loss : 1.058 Time : 120.655 seconds \n",
      "Epoch : 3 Test Acc : 61.780\n",
      "--------------------------------------------------------------\n",
      "Batch Index : 0 Loss : 0.957 Time : 1.447 seconds \n",
      "Batch Index : 100 Loss : 0.945 Time : 121.457 seconds \n",
      "Batch Index : 200 Loss : 0.928 Time : 123.048 seconds \n",
      "Batch Index : 300 Loss : 0.914 Time : 123.031 seconds \n",
      "Epoch : 4 Test Acc : 61.220\n",
      "--------------------------------------------------------------\n",
      "Batch Index : 0 Loss : 0.740 Time : 1.356 seconds \n",
      "Batch Index : 100 Loss : 0.832 Time : 122.036 seconds \n",
      "Batch Index : 200 Loss : 0.831 Time : 121.389 seconds \n",
      "Batch Index : 300 Loss : 0.825 Time : 121.724 seconds \n",
      "Epoch : 5 Test Acc : 57.500\n",
      "--------------------------------------------------------------\n",
      "Batch Index : 0 Loss : 0.705 Time : 1.366 seconds \n",
      "Batch Index : 100 Loss : 0.728 Time : 122.449 seconds \n",
      "Batch Index : 200 Loss : 0.739 Time : 123.421 seconds \n",
      "Batch Index : 300 Loss : 0.747 Time : 123.336 seconds \n",
      "Epoch : 6 Test Acc : 56.590\n",
      "--------------------------------------------------------------\n",
      "Batch Index : 0 Loss : 0.550 Time : 1.784 seconds \n",
      "Batch Index : 100 Loss : 0.713 Time : 126.156 seconds \n",
      "Batch Index : 200 Loss : 0.705 Time : 123.194 seconds \n",
      "Batch Index : 300 Loss : 0.704 Time : 125.190 seconds \n",
      "Epoch : 7 Test Acc : 67.040\n",
      "--------------------------------------------------------------\n",
      "Batch Index : 0 Loss : 0.631 Time : 1.451 seconds \n",
      "Batch Index : 100 Loss : 0.642 Time : 128.685 seconds \n",
      "Batch Index : 200 Loss : 0.649 Time : 132.308 seconds \n",
      "Batch Index : 300 Loss : 0.651 Time : 126.434 seconds \n",
      "Epoch : 8 Test Acc : 65.540\n",
      "--------------------------------------------------------------\n",
      "Batch Index : 0 Loss : 0.660 Time : 1.424 seconds \n",
      "Batch Index : 100 Loss : 0.596 Time : 119.784 seconds \n",
      "Batch Index : 200 Loss : 0.604 Time : 119.874 seconds \n",
      "Batch Index : 300 Loss : 0.612 Time : 126.399 seconds \n",
      "Epoch : 9 Test Acc : 65.060\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    scheduler.step()\n",
    "    # Train\n",
    "    start = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()                 # Zero the gradients\n",
    "\n",
    "        outputs = clf(inputs)                 # Forward pass\n",
    "        loss = criterion(outputs, targets)    # Compute the Loss\n",
    "        loss.backward()                       # Compute the Gradients\n",
    "\n",
    "        optimizer.step()                      # Updated the weights\n",
    "        losses.append(loss.item())\n",
    "        end = time.time()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "          print('Batch Index : %d Loss : %.3f Time : %.3f seconds ' % (batch_idx, np.mean(losses), end - start))\n",
    "      \n",
    "          start = time.time()\n",
    "    # Evaluate\n",
    "    clf.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "          inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "          outputs = clf(inputs)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += targets.size(0)\n",
    "          correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "      print('Epoch : %d Test Acc : %.3f' % (epoch, 100.*correct/total))\n",
    "      print('--------------------------------------------------------------')\n",
    "    clf.train()   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
