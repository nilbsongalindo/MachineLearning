{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with ANN\n",
    "\n",
    "Avila Data Set can be found in: https://archive.ics.uci.edu/ml/datasets/Avila\n",
    "\n",
    "Abstract: The Avila data set has been extracted from 800 images of the 'Avila Bible', an XII century giant Latin copy of the Bible. The prediction task consists in associating each pattern to a copyist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intercolumnar distance</th>\n",
       "      <th>upper margin</th>\n",
       "      <th>lower margin</th>\n",
       "      <th>exploitation</th>\n",
       "      <th>row number</th>\n",
       "      <th>modular ratio</th>\n",
       "      <th>interlinear spacing</th>\n",
       "      <th>weight</th>\n",
       "      <th>peak number</th>\n",
       "      <th>modular ratio/ interlinear spacing</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.266074</td>\n",
       "      <td>-0.165620</td>\n",
       "      <td>0.320980</td>\n",
       "      <td>0.483299</td>\n",
       "      <td>0.172340</td>\n",
       "      <td>0.273364</td>\n",
       "      <td>0.371178</td>\n",
       "      <td>0.929823</td>\n",
       "      <td>0.251173</td>\n",
       "      <td>0.159345</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.130292</td>\n",
       "      <td>0.870736</td>\n",
       "      <td>-3.210528</td>\n",
       "      <td>0.062493</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>1.436060</td>\n",
       "      <td>1.465940</td>\n",
       "      <td>0.636203</td>\n",
       "      <td>0.282354</td>\n",
       "      <td>0.515587</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.116585</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.068476</td>\n",
       "      <td>-0.783147</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>0.439463</td>\n",
       "      <td>-0.081827</td>\n",
       "      <td>-0.888236</td>\n",
       "      <td>-0.123005</td>\n",
       "      <td>0.582939</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031541</td>\n",
       "      <td>0.297600</td>\n",
       "      <td>-3.210528</td>\n",
       "      <td>-0.583590</td>\n",
       "      <td>-0.721442</td>\n",
       "      <td>-0.307984</td>\n",
       "      <td>0.710932</td>\n",
       "      <td>1.051693</td>\n",
       "      <td>0.594169</td>\n",
       "      <td>-0.533994</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.229043</td>\n",
       "      <td>0.807926</td>\n",
       "      <td>-0.052442</td>\n",
       "      <td>0.082634</td>\n",
       "      <td>0.261718</td>\n",
       "      <td>0.148790</td>\n",
       "      <td>0.635431</td>\n",
       "      <td>0.051062</td>\n",
       "      <td>0.032902</td>\n",
       "      <td>-0.086652</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   intercolumnar distance  upper margin  lower margin  exploitation  \\\n",
       "0                0.266074     -0.165620      0.320980      0.483299   \n",
       "1                0.130292      0.870736     -3.210528      0.062493   \n",
       "2               -0.116585      0.069915      0.068476     -0.783147   \n",
       "3                0.031541      0.297600     -3.210528     -0.583590   \n",
       "4                0.229043      0.807926     -0.052442      0.082634   \n",
       "\n",
       "   row number  modular ratio  interlinear spacing    weight  peak number  \\\n",
       "0    0.172340       0.273364             0.371178  0.929823     0.251173   \n",
       "1    0.261718       1.436060             1.465940  0.636203     0.282354   \n",
       "2    0.261718       0.439463            -0.081827 -0.888236    -0.123005   \n",
       "3   -0.721442      -0.307984             0.710932  1.051693     0.594169   \n",
       "4    0.261718       0.148790             0.635431  0.051062     0.032902   \n",
       "\n",
       "   modular ratio/ interlinear spacing Class  \n",
       "0                            0.159345     A  \n",
       "1                            0.515587     A  \n",
       "2                            0.582939     A  \n",
       "3                           -0.533994     A  \n",
       "4                           -0.086652     F  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('/data/avila-tr.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Amount of instances per class')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Data preprocessing\n",
    "\n",
    "name = ['A','F', 'E', 'I', 'X', 'H', 'G', 'D', 'Y','C', 'W', 'B']\n",
    "plt.bar(name, dataset['Class'].value_counts(), width=0.4, align='center')\n",
    "plt.ylabel('Instances')\n",
    "plt.xlabel('Classes')\n",
    "plt.title('Amount of instances per class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a sampling and saving in another csv\n",
    "my_filter2 = dataset['Class'] == 'A'\n",
    "data3 = dataset[my_filter2]\n",
    "data3 = shuffle(data3)\n",
    "data3 = data3.sample(500)\n",
    "\n",
    "my_filter2 = dataset['Class'] == 'F'\n",
    "data4 = dataset[my_filter2]\n",
    "data4 = shuffle(data4)\n",
    "data4 = data4.sample(500)\n",
    "\n",
    "my_filter2 = dataset['Class'] == 'E'\n",
    "data5 = dataset[my_filter2]\n",
    "data5 = shuffle(data5)\n",
    "data5 = data5.sample(500)\n",
    "\n",
    "my_filter2 = dataset['Class'] == 'I'\n",
    "data6 = dataset[my_filter2]\n",
    "data6 = shuffle(data6)\n",
    "data6 = data6.sample(500)\n",
    "\n",
    "my_filter2 = dataset['Class'] == 'X'\n",
    "data7 = dataset[my_filter2]\n",
    "data7 = shuffle(data7)\n",
    "data7 = data7.sample(500)\n",
    "\n",
    "my_filter2 = dataset['Class'] == 'H'\n",
    "data8 = dataset[my_filter2]\n",
    "data8 = shuffle(data8)\n",
    "data8 = data8.sample(500)\n",
    "\n",
    "my_filter2 = dataset['Class'] == 'G'\n",
    "data9 = dataset[my_filter2]\n",
    "\n",
    "my_filter2 = dataset['Class'] == 'D'\n",
    "data10 = dataset[my_filter2]\n",
    "\n",
    "my_filter2 = dataset['Class'] == 'Y'\n",
    "data11 = dataset[my_filter2]\n",
    "\n",
    "my_filter2 = dataset['Class'] == 'C'\n",
    "data12 = dataset[my_filter2]\n",
    "\n",
    "my_filter2 = dataset['Class'] == 'W'\n",
    "data13 = dataset[my_filter2]\n",
    "\n",
    "my_filter2 = dataset['Class'] == 'B'\n",
    "data14 = dataset[my_filter2]\n",
    "\n",
    "final = pd.concat([data3, data4, data5, data6, data7, data8,data9, data10,data11, data12, data13, data14], ignore_index = True, axis = 0)\n",
    "\n",
    "final.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating data\n",
    "final = pd.read_csv('/data/final.csv')\n",
    "X = final.iloc[:, 1:-1].values\n",
    "y = final.iloc[:, 11].values\n",
    "\n",
    "d = {'G':500, 'D':500,'Y':500, 'C':500, 'W':500, 'B':500}\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_res, y_res = SMOTE(ratio=d, k_neighbors=4).fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({'A': 500, 'F': 500, 'E': 500, 'I': 500, 'X': 500, 'H': 500, 'G': 500, 'D': 500, 'Y': 500, 'C': 500, 'W': 500, 'B': 500})\n"
     ]
    }
   ],
   "source": [
    "#Now every class have the same amount of instances\n",
    "from collections import Counter\n",
    "print('Original dataset shape {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nilbson/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nilbson/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 3372 samples, validate on 844 samples\n",
      "Epoch 1/300\n",
      "3372/3372 [==============================] - 0s 113us/step - loss: 2.4174 - accuracy: 0.1329 - val_loss: 2.3025 - val_accuracy: 0.1197\n",
      "Epoch 2/300\n",
      "3372/3372 [==============================] - 0s 28us/step - loss: 2.2802 - accuracy: 0.1260 - val_loss: 2.2587 - val_accuracy: 0.1303\n",
      "Epoch 3/300\n",
      "3372/3372 [==============================] - 0s 29us/step - loss: 2.2203 - accuracy: 0.1572 - val_loss: 2.1708 - val_accuracy: 0.1777\n",
      "Epoch 4/300\n",
      "  32/3372 [..............................] - ETA: 0s - loss: 2.1821 - accuracy: 0.1875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nilbson/.local/lib/python3.7/site-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3372/3372 [==============================] - 0s 27us/step - loss: 2.0752 - accuracy: 0.2198 - val_loss: 1.9828 - val_accuracy: 0.2393\n",
      "Epoch 5/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.9478 - accuracy: 0.2372 - val_loss: 1.8997 - val_accuracy: 0.2464\n",
      "Epoch 6/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.8988 - accuracy: 0.2497 - val_loss: 1.8674 - val_accuracy: 0.2346\n",
      "Epoch 7/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.8768 - accuracy: 0.2476 - val_loss: 1.8485 - val_accuracy: 0.2524\n",
      "Epoch 8/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.8604 - accuracy: 0.2630 - val_loss: 1.8312 - val_accuracy: 0.2654\n",
      "Epoch 9/300\n",
      "3372/3372 [==============================] - 0s 28us/step - loss: 1.8415 - accuracy: 0.2811 - val_loss: 1.8070 - val_accuracy: 0.3116\n",
      "Epoch 10/300\n",
      "3372/3372 [==============================] - 0s 28us/step - loss: 1.8159 - accuracy: 0.3215 - val_loss: 1.7889 - val_accuracy: 0.3104\n",
      "Epoch 11/300\n",
      "3372/3372 [==============================] - 0s 28us/step - loss: 1.7918 - accuracy: 0.3283 - val_loss: 1.7623 - val_accuracy: 0.3187\n",
      "Epoch 12/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.7685 - accuracy: 0.3330 - val_loss: 1.7398 - val_accuracy: 0.3306\n",
      "Epoch 13/300\n",
      "3372/3372 [==============================] - 0s 28us/step - loss: 1.7443 - accuracy: 0.3425 - val_loss: 1.7272 - val_accuracy: 0.3424\n",
      "Epoch 14/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.7218 - accuracy: 0.3553 - val_loss: 1.7095 - val_accuracy: 0.3555\n",
      "Epoch 15/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.6979 - accuracy: 0.3713 - val_loss: 1.6793 - val_accuracy: 0.3590\n",
      "Epoch 16/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.6739 - accuracy: 0.3722 - val_loss: 1.6524 - val_accuracy: 0.3756\n",
      "Epoch 17/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.6471 - accuracy: 0.3805 - val_loss: 1.6306 - val_accuracy: 0.3898\n",
      "Epoch 18/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.6194 - accuracy: 0.3870 - val_loss: 1.5994 - val_accuracy: 0.3898\n",
      "Epoch 19/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.5917 - accuracy: 0.3983 - val_loss: 1.5729 - val_accuracy: 0.4159\n",
      "Epoch 20/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.5645 - accuracy: 0.4093 - val_loss: 1.5480 - val_accuracy: 0.4254\n",
      "Epoch 21/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.5409 - accuracy: 0.4220 - val_loss: 1.5255 - val_accuracy: 0.4360\n",
      "Epoch 22/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.5195 - accuracy: 0.4294 - val_loss: 1.5045 - val_accuracy: 0.4455\n",
      "Epoch 23/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.4977 - accuracy: 0.4365 - val_loss: 1.4824 - val_accuracy: 0.4585\n",
      "Epoch 24/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.4810 - accuracy: 0.4377 - val_loss: 1.4634 - val_accuracy: 0.4562\n",
      "Epoch 25/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 1.4630 - accuracy: 0.4440 - val_loss: 1.4530 - val_accuracy: 0.4562\n",
      "Epoch 26/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.4477 - accuracy: 0.4570 - val_loss: 1.4394 - val_accuracy: 0.4550\n",
      "Epoch 27/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 1.4329 - accuracy: 0.4594 - val_loss: 1.4281 - val_accuracy: 0.4656\n",
      "Epoch 28/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.4184 - accuracy: 0.4650 - val_loss: 1.4249 - val_accuracy: 0.4692\n",
      "Epoch 29/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.4040 - accuracy: 0.4760 - val_loss: 1.4044 - val_accuracy: 0.4597\n",
      "Epoch 30/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 1.3902 - accuracy: 0.4798 - val_loss: 1.3854 - val_accuracy: 0.4668\n",
      "Epoch 31/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.3770 - accuracy: 0.4825 - val_loss: 1.3786 - val_accuracy: 0.4775\n",
      "Epoch 32/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.3619 - accuracy: 0.4956 - val_loss: 1.3675 - val_accuracy: 0.4834\n",
      "Epoch 33/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 1.3500 - accuracy: 0.4994 - val_loss: 1.3595 - val_accuracy: 0.4882\n",
      "Epoch 34/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 1.3385 - accuracy: 0.5033 - val_loss: 1.3563 - val_accuracy: 0.4988\n",
      "Epoch 35/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.3292 - accuracy: 0.5130 - val_loss: 1.3448 - val_accuracy: 0.4929\n",
      "Epoch 36/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 1.3200 - accuracy: 0.5163 - val_loss: 1.3359 - val_accuracy: 0.5107\n",
      "Epoch 37/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.3097 - accuracy: 0.5243 - val_loss: 1.3292 - val_accuracy: 0.5190\n",
      "Epoch 38/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.3024 - accuracy: 0.5261 - val_loss: 1.3212 - val_accuracy: 0.5225\n",
      "Epoch 39/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.2924 - accuracy: 0.5282 - val_loss: 1.3248 - val_accuracy: 0.5225\n",
      "Epoch 40/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.2846 - accuracy: 0.5359 - val_loss: 1.3074 - val_accuracy: 0.5249\n",
      "Epoch 41/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.2761 - accuracy: 0.5448 - val_loss: 1.3081 - val_accuracy: 0.5284\n",
      "Epoch 42/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.2709 - accuracy: 0.5406 - val_loss: 1.3061 - val_accuracy: 0.5166\n",
      "Epoch 43/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.2634 - accuracy: 0.5513 - val_loss: 1.3056 - val_accuracy: 0.5213\n",
      "Epoch 44/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.2544 - accuracy: 0.5569 - val_loss: 1.2934 - val_accuracy: 0.5249\n",
      "Epoch 45/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.2470 - accuracy: 0.5578 - val_loss: 1.2919 - val_accuracy: 0.5296\n",
      "Epoch 46/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.2406 - accuracy: 0.5602 - val_loss: 1.2849 - val_accuracy: 0.5367\n",
      "Epoch 47/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.2309 - accuracy: 0.5685 - val_loss: 1.2772 - val_accuracy: 0.5415\n",
      "Epoch 48/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.2224 - accuracy: 0.5694 - val_loss: 1.2762 - val_accuracy: 0.5427\n",
      "Epoch 49/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.2136 - accuracy: 0.5762 - val_loss: 1.2587 - val_accuracy: 0.5616\n",
      "Epoch 50/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.2057 - accuracy: 0.5780 - val_loss: 1.2637 - val_accuracy: 0.5581\n",
      "Epoch 51/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.1940 - accuracy: 0.5824 - val_loss: 1.2560 - val_accuracy: 0.5652\n",
      "Epoch 52/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.1889 - accuracy: 0.5810 - val_loss: 1.2413 - val_accuracy: 0.5628\n",
      "Epoch 53/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.1765 - accuracy: 0.5881 - val_loss: 1.2370 - val_accuracy: 0.5664\n",
      "Epoch 54/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.1697 - accuracy: 0.5919 - val_loss: 1.2361 - val_accuracy: 0.5675\n",
      "Epoch 55/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 1.1596 - accuracy: 0.5955 - val_loss: 1.2230 - val_accuracy: 0.5711\n",
      "Epoch 56/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 1.1505 - accuracy: 0.5934 - val_loss: 1.2072 - val_accuracy: 0.5794\n",
      "Epoch 57/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.1427 - accuracy: 0.6017 - val_loss: 1.2056 - val_accuracy: 0.5770\n",
      "Epoch 58/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 1.1334 - accuracy: 0.6026 - val_loss: 1.1894 - val_accuracy: 0.5794\n",
      "Epoch 59/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 1.1242 - accuracy: 0.6053 - val_loss: 1.1893 - val_accuracy: 0.5841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.1162 - accuracy: 0.6077 - val_loss: 1.1794 - val_accuracy: 0.5877\n",
      "Epoch 61/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.1111 - accuracy: 0.6121 - val_loss: 1.1730 - val_accuracy: 0.5912\n",
      "Epoch 62/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.1014 - accuracy: 0.6139 - val_loss: 1.1677 - val_accuracy: 0.5948\n",
      "Epoch 63/300\n",
      "3372/3372 [==============================] - 0s 30us/step - loss: 1.0946 - accuracy: 0.6168 - val_loss: 1.1581 - val_accuracy: 0.6019\n",
      "Epoch 64/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.0877 - accuracy: 0.6177 - val_loss: 1.1512 - val_accuracy: 0.6102\n",
      "Epoch 65/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.0802 - accuracy: 0.6180 - val_loss: 1.1490 - val_accuracy: 0.6066\n",
      "Epoch 66/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.0743 - accuracy: 0.6183 - val_loss: 1.1479 - val_accuracy: 0.6090\n",
      "Epoch 67/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.0677 - accuracy: 0.6177 - val_loss: 1.1333 - val_accuracy: 0.6137\n",
      "Epoch 68/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.0599 - accuracy: 0.6290 - val_loss: 1.1388 - val_accuracy: 0.6149\n",
      "Epoch 69/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 1.0572 - accuracy: 0.6249 - val_loss: 1.1239 - val_accuracy: 0.6173\n",
      "Epoch 70/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.0483 - accuracy: 0.6266 - val_loss: 1.1126 - val_accuracy: 0.6173\n",
      "Epoch 71/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.0433 - accuracy: 0.6293 - val_loss: 1.1174 - val_accuracy: 0.6149\n",
      "Epoch 72/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.0370 - accuracy: 0.6311 - val_loss: 1.1122 - val_accuracy: 0.6102\n",
      "Epoch 73/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.0310 - accuracy: 0.6305 - val_loss: 1.1177 - val_accuracy: 0.6114\n",
      "Epoch 74/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.0252 - accuracy: 0.6335 - val_loss: 1.0982 - val_accuracy: 0.6149\n",
      "Epoch 75/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.0196 - accuracy: 0.6370 - val_loss: 1.1087 - val_accuracy: 0.6114\n",
      "Epoch 76/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 1.0145 - accuracy: 0.6329 - val_loss: 1.0860 - val_accuracy: 0.6173\n",
      "Epoch 77/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 1.0102 - accuracy: 0.6358 - val_loss: 1.0909 - val_accuracy: 0.6197\n",
      "Epoch 78/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 1.0063 - accuracy: 0.6415 - val_loss: 1.0892 - val_accuracy: 0.6185\n",
      "Epoch 79/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9991 - accuracy: 0.6423 - val_loss: 1.0733 - val_accuracy: 0.6209\n",
      "Epoch 80/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9940 - accuracy: 0.6438 - val_loss: 1.0707 - val_accuracy: 0.6209\n",
      "Epoch 81/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9889 - accuracy: 0.6489 - val_loss: 1.0730 - val_accuracy: 0.6161\n",
      "Epoch 82/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9834 - accuracy: 0.6468 - val_loss: 1.0694 - val_accuracy: 0.6256\n",
      "Epoch 83/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.9796 - accuracy: 0.6515 - val_loss: 1.0720 - val_accuracy: 0.6280\n",
      "Epoch 84/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9747 - accuracy: 0.6530 - val_loss: 1.0539 - val_accuracy: 0.6244\n",
      "Epoch 85/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.9697 - accuracy: 0.6509 - val_loss: 1.0694 - val_accuracy: 0.6209\n",
      "Epoch 86/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9682 - accuracy: 0.6527 - val_loss: 1.0504 - val_accuracy: 0.6280\n",
      "Epoch 87/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9632 - accuracy: 0.6524 - val_loss: 1.0552 - val_accuracy: 0.6268\n",
      "Epoch 88/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9603 - accuracy: 0.6474 - val_loss: 1.0477 - val_accuracy: 0.6256\n",
      "Epoch 89/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.9558 - accuracy: 0.6554 - val_loss: 1.0378 - val_accuracy: 0.6339\n",
      "Epoch 90/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.9507 - accuracy: 0.6563 - val_loss: 1.0456 - val_accuracy: 0.6244\n",
      "Epoch 91/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.9485 - accuracy: 0.6590 - val_loss: 1.0411 - val_accuracy: 0.6374\n",
      "Epoch 92/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.9436 - accuracy: 0.6587 - val_loss: 1.0368 - val_accuracy: 0.6256\n",
      "Epoch 93/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9403 - accuracy: 0.6560 - val_loss: 1.0304 - val_accuracy: 0.6220\n",
      "Epoch 94/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9388 - accuracy: 0.6581 - val_loss: 1.0238 - val_accuracy: 0.6363\n",
      "Epoch 95/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.9332 - accuracy: 0.6575 - val_loss: 1.0345 - val_accuracy: 0.6256\n",
      "Epoch 96/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9316 - accuracy: 0.6619 - val_loss: 1.0141 - val_accuracy: 0.6303\n",
      "Epoch 97/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.9276 - accuracy: 0.6584 - val_loss: 1.0212 - val_accuracy: 0.6327\n",
      "Epoch 98/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9247 - accuracy: 0.6634 - val_loss: 1.0368 - val_accuracy: 0.6268\n",
      "Epoch 99/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9233 - accuracy: 0.6643 - val_loss: 1.0190 - val_accuracy: 0.6303\n",
      "Epoch 100/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.9208 - accuracy: 0.6628 - val_loss: 1.0104 - val_accuracy: 0.6303\n",
      "Epoch 101/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.9180 - accuracy: 0.6604 - val_loss: 1.0102 - val_accuracy: 0.6374\n",
      "Epoch 102/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.9124 - accuracy: 0.6625 - val_loss: 1.0088 - val_accuracy: 0.6327\n",
      "Epoch 103/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.9099 - accuracy: 0.6655 - val_loss: 1.0115 - val_accuracy: 0.6374\n",
      "Epoch 104/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.9071 - accuracy: 0.6673 - val_loss: 0.9971 - val_accuracy: 0.6339\n",
      "Epoch 105/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.9036 - accuracy: 0.6696 - val_loss: 1.0063 - val_accuracy: 0.6315\n",
      "Epoch 106/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.9001 - accuracy: 0.6679 - val_loss: 1.0147 - val_accuracy: 0.6434\n",
      "Epoch 107/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8997 - accuracy: 0.6699 - val_loss: 1.0133 - val_accuracy: 0.6434\n",
      "Epoch 108/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.8953 - accuracy: 0.6741 - val_loss: 0.9909 - val_accuracy: 0.6351\n",
      "Epoch 109/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8937 - accuracy: 0.6673 - val_loss: 0.9951 - val_accuracy: 0.6351\n",
      "Epoch 110/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8928 - accuracy: 0.6708 - val_loss: 0.9912 - val_accuracy: 0.6434\n",
      "Epoch 111/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.8898 - accuracy: 0.6720 - val_loss: 0.9923 - val_accuracy: 0.6363\n",
      "Epoch 112/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8857 - accuracy: 0.6762 - val_loss: 0.9813 - val_accuracy: 0.6422\n",
      "Epoch 113/300\n",
      "3372/3372 [==============================] - 0s 28us/step - loss: 0.8821 - accuracy: 0.6770 - val_loss: 0.9771 - val_accuracy: 0.6422\n",
      "Epoch 114/300\n",
      "3372/3372 [==============================] - 0s 28us/step - loss: 0.8801 - accuracy: 0.6845 - val_loss: 0.9892 - val_accuracy: 0.6434\n",
      "Epoch 115/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8772 - accuracy: 0.6821 - val_loss: 0.9856 - val_accuracy: 0.6481\n",
      "Epoch 116/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8776 - accuracy: 0.6809 - val_loss: 0.9900 - val_accuracy: 0.6410\n",
      "Epoch 117/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8722 - accuracy: 0.6853 - val_loss: 0.9659 - val_accuracy: 0.6528\n",
      "Epoch 118/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8691 - accuracy: 0.6800 - val_loss: 0.9715 - val_accuracy: 0.6540\n",
      "Epoch 119/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8668 - accuracy: 0.6874 - val_loss: 0.9696 - val_accuracy: 0.6576\n",
      "Epoch 120/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8656 - accuracy: 0.6874 - val_loss: 0.9684 - val_accuracy: 0.6552\n",
      "Epoch 121/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8628 - accuracy: 0.6922 - val_loss: 0.9615 - val_accuracy: 0.6505\n",
      "Epoch 122/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8619 - accuracy: 0.6904 - val_loss: 0.9578 - val_accuracy: 0.6540\n",
      "Epoch 123/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8583 - accuracy: 0.6907 - val_loss: 0.9560 - val_accuracy: 0.6552\n",
      "Epoch 124/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8567 - accuracy: 0.6942 - val_loss: 0.9428 - val_accuracy: 0.6588\n",
      "Epoch 125/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.8524 - accuracy: 0.6907 - val_loss: 0.9486 - val_accuracy: 0.6564\n",
      "Epoch 126/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8544 - accuracy: 0.6895 - val_loss: 0.9384 - val_accuracy: 0.6600\n",
      "Epoch 127/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8488 - accuracy: 0.6975 - val_loss: 0.9469 - val_accuracy: 0.6528\n",
      "Epoch 128/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8454 - accuracy: 0.6972 - val_loss: 0.9378 - val_accuracy: 0.6647\n",
      "Epoch 129/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8454 - accuracy: 0.6931 - val_loss: 0.9392 - val_accuracy: 0.6611\n",
      "Epoch 130/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8441 - accuracy: 0.6975 - val_loss: 0.9488 - val_accuracy: 0.6540\n",
      "Epoch 131/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8404 - accuracy: 0.6963 - val_loss: 0.9371 - val_accuracy: 0.6623\n",
      "Epoch 132/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.8360 - accuracy: 0.7002 - val_loss: 0.9411 - val_accuracy: 0.6694\n",
      "Epoch 133/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8358 - accuracy: 0.6999 - val_loss: 0.9440 - val_accuracy: 0.6600\n",
      "Epoch 134/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8340 - accuracy: 0.6993 - val_loss: 0.9302 - val_accuracy: 0.6600\n",
      "Epoch 135/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8320 - accuracy: 0.7061 - val_loss: 0.9348 - val_accuracy: 0.6635\n",
      "Epoch 136/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8301 - accuracy: 0.7005 - val_loss: 0.9267 - val_accuracy: 0.6706\n",
      "Epoch 137/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8277 - accuracy: 0.7070 - val_loss: 0.9361 - val_accuracy: 0.6588\n",
      "Epoch 138/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8262 - accuracy: 0.7037 - val_loss: 0.9298 - val_accuracy: 0.6505\n",
      "Epoch 139/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8244 - accuracy: 0.7076 - val_loss: 0.9221 - val_accuracy: 0.6635\n",
      "Epoch 140/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8242 - accuracy: 0.7076 - val_loss: 0.9210 - val_accuracy: 0.6671\n",
      "Epoch 141/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8232 - accuracy: 0.7037 - val_loss: 0.9258 - val_accuracy: 0.6635\n",
      "Epoch 142/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8211 - accuracy: 0.7091 - val_loss: 0.9175 - val_accuracy: 0.6754\n",
      "Epoch 143/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.8185 - accuracy: 0.7112 - val_loss: 0.9152 - val_accuracy: 0.6742\n",
      "Epoch 144/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8159 - accuracy: 0.7109 - val_loss: 0.9159 - val_accuracy: 0.6659\n",
      "Epoch 145/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8146 - accuracy: 0.7100 - val_loss: 0.9194 - val_accuracy: 0.6671\n",
      "Epoch 146/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8115 - accuracy: 0.7132 - val_loss: 0.9135 - val_accuracy: 0.6647\n",
      "Epoch 147/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8126 - accuracy: 0.7100 - val_loss: 0.9133 - val_accuracy: 0.6730\n",
      "Epoch 148/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8104 - accuracy: 0.7117 - val_loss: 0.9253 - val_accuracy: 0.6647\n",
      "Epoch 149/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8084 - accuracy: 0.7138 - val_loss: 0.9208 - val_accuracy: 0.6635\n",
      "Epoch 150/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8083 - accuracy: 0.7153 - val_loss: 0.9183 - val_accuracy: 0.6765\n",
      "Epoch 151/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8072 - accuracy: 0.7171 - val_loss: 0.9103 - val_accuracy: 0.6647\n",
      "Epoch 152/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8026 - accuracy: 0.7174 - val_loss: 0.9177 - val_accuracy: 0.6730\n",
      "Epoch 153/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.8022 - accuracy: 0.7147 - val_loss: 0.9183 - val_accuracy: 0.6730\n",
      "Epoch 154/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.8016 - accuracy: 0.7144 - val_loss: 0.9063 - val_accuracy: 0.6706\n",
      "Epoch 155/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7989 - accuracy: 0.7150 - val_loss: 0.9117 - val_accuracy: 0.6671\n",
      "Epoch 156/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7980 - accuracy: 0.7120 - val_loss: 0.9048 - val_accuracy: 0.6742\n",
      "Epoch 157/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7958 - accuracy: 0.7183 - val_loss: 0.9076 - val_accuracy: 0.6765\n",
      "Epoch 158/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7946 - accuracy: 0.7221 - val_loss: 0.9121 - val_accuracy: 0.6754\n",
      "Epoch 159/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7947 - accuracy: 0.7174 - val_loss: 0.9007 - val_accuracy: 0.6718\n",
      "Epoch 160/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7948 - accuracy: 0.7159 - val_loss: 0.9009 - val_accuracy: 0.6754\n",
      "Epoch 161/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7913 - accuracy: 0.7192 - val_loss: 0.8986 - val_accuracy: 0.6730\n",
      "Epoch 162/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7896 - accuracy: 0.7215 - val_loss: 0.9065 - val_accuracy: 0.6671\n",
      "Epoch 163/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7888 - accuracy: 0.7168 - val_loss: 0.9078 - val_accuracy: 0.6647\n",
      "Epoch 164/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7864 - accuracy: 0.7177 - val_loss: 0.8964 - val_accuracy: 0.6730\n",
      "Epoch 165/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7857 - accuracy: 0.7209 - val_loss: 0.8930 - val_accuracy: 0.6706\n",
      "Epoch 166/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7853 - accuracy: 0.7227 - val_loss: 0.9111 - val_accuracy: 0.6682\n",
      "Epoch 167/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7851 - accuracy: 0.7227 - val_loss: 0.9015 - val_accuracy: 0.6706\n",
      "Epoch 168/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7824 - accuracy: 0.7245 - val_loss: 0.9035 - val_accuracy: 0.6706\n",
      "Epoch 169/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7820 - accuracy: 0.7257 - val_loss: 0.9169 - val_accuracy: 0.6635\n",
      "Epoch 170/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7822 - accuracy: 0.7215 - val_loss: 0.8992 - val_accuracy: 0.6765\n",
      "Epoch 171/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7783 - accuracy: 0.7224 - val_loss: 0.9104 - val_accuracy: 0.6694\n",
      "Epoch 172/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7786 - accuracy: 0.7292 - val_loss: 0.8921 - val_accuracy: 0.6694\n",
      "Epoch 173/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7770 - accuracy: 0.7245 - val_loss: 0.8898 - val_accuracy: 0.6718\n",
      "Epoch 174/300\n",
      "3372/3372 [==============================] - 0s 28us/step - loss: 0.7770 - accuracy: 0.7209 - val_loss: 0.8997 - val_accuracy: 0.6765\n",
      "Epoch 175/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7743 - accuracy: 0.7236 - val_loss: 0.9017 - val_accuracy: 0.6789\n",
      "Epoch 176/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7758 - accuracy: 0.7230 - val_loss: 0.8877 - val_accuracy: 0.6682\n",
      "Epoch 177/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7741 - accuracy: 0.7266 - val_loss: 0.8881 - val_accuracy: 0.6706\n",
      "Epoch 178/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7708 - accuracy: 0.7310 - val_loss: 0.8896 - val_accuracy: 0.6718\n",
      "Epoch 179/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7723 - accuracy: 0.7212 - val_loss: 0.9013 - val_accuracy: 0.6694\n",
      "Epoch 180/300\n",
      "3372/3372 [==============================] - 0s 28us/step - loss: 0.7716 - accuracy: 0.7209 - val_loss: 0.8847 - val_accuracy: 0.6706\n",
      "Epoch 181/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7664 - accuracy: 0.7272 - val_loss: 0.8901 - val_accuracy: 0.6694\n",
      "Epoch 182/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7668 - accuracy: 0.7254 - val_loss: 0.8903 - val_accuracy: 0.6694\n",
      "Epoch 183/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7688 - accuracy: 0.7301 - val_loss: 0.8814 - val_accuracy: 0.6706\n",
      "Epoch 184/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7635 - accuracy: 0.7292 - val_loss: 0.8879 - val_accuracy: 0.6730\n",
      "Epoch 185/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7638 - accuracy: 0.7266 - val_loss: 0.8822 - val_accuracy: 0.6742\n",
      "Epoch 186/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7638 - accuracy: 0.7281 - val_loss: 0.8752 - val_accuracy: 0.6730\n",
      "Epoch 187/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7626 - accuracy: 0.7269 - val_loss: 0.8817 - val_accuracy: 0.6730\n",
      "Epoch 188/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7611 - accuracy: 0.7275 - val_loss: 0.8835 - val_accuracy: 0.6671\n",
      "Epoch 189/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7581 - accuracy: 0.7284 - val_loss: 0.8827 - val_accuracy: 0.6765\n",
      "Epoch 190/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7582 - accuracy: 0.7281 - val_loss: 0.8818 - val_accuracy: 0.6706\n",
      "Epoch 191/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7573 - accuracy: 0.7316 - val_loss: 0.8789 - val_accuracy: 0.6754\n",
      "Epoch 192/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7575 - accuracy: 0.7310 - val_loss: 0.8784 - val_accuracy: 0.6754\n",
      "Epoch 193/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7571 - accuracy: 0.7301 - val_loss: 0.8713 - val_accuracy: 0.6706\n",
      "Epoch 194/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7561 - accuracy: 0.7316 - val_loss: 0.8704 - val_accuracy: 0.6777\n",
      "Epoch 195/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7534 - accuracy: 0.7292 - val_loss: 0.8752 - val_accuracy: 0.6754\n",
      "Epoch 196/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7538 - accuracy: 0.7301 - val_loss: 0.8757 - val_accuracy: 0.6671\n",
      "Epoch 197/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7515 - accuracy: 0.7284 - val_loss: 0.8859 - val_accuracy: 0.6789\n",
      "Epoch 198/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7522 - accuracy: 0.7328 - val_loss: 0.8746 - val_accuracy: 0.6706\n",
      "Epoch 199/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7511 - accuracy: 0.7292 - val_loss: 0.8780 - val_accuracy: 0.6694\n",
      "Epoch 200/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7495 - accuracy: 0.7349 - val_loss: 0.8771 - val_accuracy: 0.6742\n",
      "Epoch 201/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7490 - accuracy: 0.7313 - val_loss: 0.8766 - val_accuracy: 0.6765\n",
      "Epoch 202/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7489 - accuracy: 0.7298 - val_loss: 0.8677 - val_accuracy: 0.6765\n",
      "Epoch 203/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7460 - accuracy: 0.7307 - val_loss: 0.8669 - val_accuracy: 0.6742\n",
      "Epoch 204/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7478 - accuracy: 0.7331 - val_loss: 0.8727 - val_accuracy: 0.6718\n",
      "Epoch 205/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7436 - accuracy: 0.7322 - val_loss: 0.8618 - val_accuracy: 0.6789\n",
      "Epoch 206/300\n",
      "3372/3372 [==============================] - 0s 28us/step - loss: 0.7435 - accuracy: 0.7364 - val_loss: 0.8644 - val_accuracy: 0.6718\n",
      "Epoch 207/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7465 - accuracy: 0.7313 - val_loss: 0.8673 - val_accuracy: 0.6730\n",
      "Epoch 208/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7439 - accuracy: 0.7316 - val_loss: 0.8765 - val_accuracy: 0.6730\n",
      "Epoch 209/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7421 - accuracy: 0.7364 - val_loss: 0.8703 - val_accuracy: 0.6765\n",
      "Epoch 210/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7411 - accuracy: 0.7340 - val_loss: 0.8731 - val_accuracy: 0.6671\n",
      "Epoch 211/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7397 - accuracy: 0.7343 - val_loss: 0.8735 - val_accuracy: 0.6765\n",
      "Epoch 212/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7394 - accuracy: 0.7361 - val_loss: 0.8587 - val_accuracy: 0.6848\n",
      "Epoch 213/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7387 - accuracy: 0.7346 - val_loss: 0.8557 - val_accuracy: 0.6813\n",
      "Epoch 214/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7384 - accuracy: 0.7340 - val_loss: 0.8577 - val_accuracy: 0.6836\n",
      "Epoch 215/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7398 - accuracy: 0.7322 - val_loss: 0.8622 - val_accuracy: 0.6765\n",
      "Epoch 216/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7372 - accuracy: 0.7307 - val_loss: 0.8706 - val_accuracy: 0.6765\n",
      "Epoch 217/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7358 - accuracy: 0.7372 - val_loss: 0.8638 - val_accuracy: 0.6777\n",
      "Epoch 218/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7333 - accuracy: 0.7384 - val_loss: 0.8504 - val_accuracy: 0.6813\n",
      "Epoch 219/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7337 - accuracy: 0.7337 - val_loss: 0.8581 - val_accuracy: 0.6825\n",
      "Epoch 220/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7320 - accuracy: 0.7364 - val_loss: 0.8687 - val_accuracy: 0.6836\n",
      "Epoch 221/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7319 - accuracy: 0.7364 - val_loss: 0.8546 - val_accuracy: 0.6765\n",
      "Epoch 222/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7312 - accuracy: 0.7411 - val_loss: 0.8736 - val_accuracy: 0.6742\n",
      "Epoch 223/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7302 - accuracy: 0.7343 - val_loss: 0.8521 - val_accuracy: 0.6919\n",
      "Epoch 224/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7292 - accuracy: 0.7399 - val_loss: 0.8636 - val_accuracy: 0.6754\n",
      "Epoch 225/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7279 - accuracy: 0.7367 - val_loss: 0.8510 - val_accuracy: 0.6789\n",
      "Epoch 226/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7276 - accuracy: 0.7372 - val_loss: 0.8508 - val_accuracy: 0.6801\n",
      "Epoch 227/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7257 - accuracy: 0.7414 - val_loss: 0.8586 - val_accuracy: 0.6836\n",
      "Epoch 228/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7248 - accuracy: 0.7390 - val_loss: 0.8697 - val_accuracy: 0.6836\n",
      "Epoch 229/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7279 - accuracy: 0.7375 - val_loss: 0.8547 - val_accuracy: 0.6860\n",
      "Epoch 230/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7256 - accuracy: 0.7381 - val_loss: 0.8496 - val_accuracy: 0.6931\n",
      "Epoch 231/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7261 - accuracy: 0.7337 - val_loss: 0.8559 - val_accuracy: 0.6860\n",
      "Epoch 232/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7222 - accuracy: 0.7444 - val_loss: 0.8583 - val_accuracy: 0.6801\n",
      "Epoch 233/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7245 - accuracy: 0.7381 - val_loss: 0.8524 - val_accuracy: 0.6730\n",
      "Epoch 234/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7216 - accuracy: 0.7396 - val_loss: 0.8527 - val_accuracy: 0.6896\n",
      "Epoch 235/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7198 - accuracy: 0.7438 - val_loss: 0.8480 - val_accuracy: 0.6813\n",
      "Epoch 236/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7214 - accuracy: 0.7340 - val_loss: 0.8447 - val_accuracy: 0.6919\n",
      "Epoch 237/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7217 - accuracy: 0.7375 - val_loss: 0.8453 - val_accuracy: 0.6884\n",
      "Epoch 238/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7182 - accuracy: 0.7420 - val_loss: 0.8487 - val_accuracy: 0.6836\n",
      "Epoch 239/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7173 - accuracy: 0.7414 - val_loss: 0.8486 - val_accuracy: 0.6872\n",
      "Epoch 240/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7171 - accuracy: 0.7390 - val_loss: 0.8428 - val_accuracy: 0.6919\n",
      "Epoch 241/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7172 - accuracy: 0.7390 - val_loss: 0.8491 - val_accuracy: 0.6872\n",
      "Epoch 242/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7141 - accuracy: 0.7423 - val_loss: 0.8442 - val_accuracy: 0.6979\n",
      "Epoch 243/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7143 - accuracy: 0.7456 - val_loss: 0.8451 - val_accuracy: 0.6825\n",
      "Epoch 244/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.7135 - accuracy: 0.7411 - val_loss: 0.8548 - val_accuracy: 0.6872\n",
      "Epoch 245/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7143 - accuracy: 0.7426 - val_loss: 0.8451 - val_accuracy: 0.6919\n",
      "Epoch 246/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7108 - accuracy: 0.7438 - val_loss: 0.8430 - val_accuracy: 0.6931\n",
      "Epoch 247/300\n",
      "3372/3372 [==============================] - 0s 28us/step - loss: 0.7104 - accuracy: 0.7405 - val_loss: 0.8381 - val_accuracy: 0.6991\n",
      "Epoch 248/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7134 - accuracy: 0.7384 - val_loss: 0.8420 - val_accuracy: 0.6991\n",
      "Epoch 249/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7094 - accuracy: 0.7411 - val_loss: 0.8474 - val_accuracy: 0.6860\n",
      "Epoch 250/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7080 - accuracy: 0.7447 - val_loss: 0.8337 - val_accuracy: 0.6979\n",
      "Epoch 251/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7088 - accuracy: 0.7432 - val_loss: 0.8338 - val_accuracy: 0.7002\n",
      "Epoch 252/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7083 - accuracy: 0.7408 - val_loss: 0.8328 - val_accuracy: 0.6896\n",
      "Epoch 253/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7064 - accuracy: 0.7432 - val_loss: 0.8306 - val_accuracy: 0.6967\n",
      "Epoch 254/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7059 - accuracy: 0.7441 - val_loss: 0.8375 - val_accuracy: 0.6908\n",
      "Epoch 255/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7051 - accuracy: 0.7399 - val_loss: 0.8391 - val_accuracy: 0.7002\n",
      "Epoch 256/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7050 - accuracy: 0.7429 - val_loss: 0.8314 - val_accuracy: 0.6872\n",
      "Epoch 257/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7056 - accuracy: 0.7387 - val_loss: 0.8312 - val_accuracy: 0.6884\n",
      "Epoch 258/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7041 - accuracy: 0.7426 - val_loss: 0.8394 - val_accuracy: 0.6991\n",
      "Epoch 259/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.7018 - accuracy: 0.7423 - val_loss: 0.8298 - val_accuracy: 0.6955\n",
      "Epoch 260/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7019 - accuracy: 0.7423 - val_loss: 0.8346 - val_accuracy: 0.6967\n",
      "Epoch 261/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7011 - accuracy: 0.7429 - val_loss: 0.8344 - val_accuracy: 0.7026\n",
      "Epoch 262/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.7006 - accuracy: 0.7456 - val_loss: 0.8261 - val_accuracy: 0.6919\n",
      "Epoch 263/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6998 - accuracy: 0.7414 - val_loss: 0.8352 - val_accuracy: 0.6979\n",
      "Epoch 264/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6971 - accuracy: 0.7423 - val_loss: 0.8290 - val_accuracy: 0.7002\n",
      "Epoch 265/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6985 - accuracy: 0.7482 - val_loss: 0.8224 - val_accuracy: 0.7026\n",
      "Epoch 266/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.6972 - accuracy: 0.7464 - val_loss: 0.8281 - val_accuracy: 0.7002\n",
      "Epoch 267/300\n",
      "3372/3372 [==============================] - 0s 28us/step - loss: 0.6962 - accuracy: 0.7453 - val_loss: 0.8353 - val_accuracy: 0.6931\n",
      "Epoch 268/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.6954 - accuracy: 0.7461 - val_loss: 0.8297 - val_accuracy: 0.6979\n",
      "Epoch 269/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.6944 - accuracy: 0.7447 - val_loss: 0.8268 - val_accuracy: 0.7062\n",
      "Epoch 270/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6957 - accuracy: 0.7432 - val_loss: 0.8359 - val_accuracy: 0.6991\n",
      "Epoch 271/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6952 - accuracy: 0.7467 - val_loss: 0.8358 - val_accuracy: 0.6943\n",
      "Epoch 272/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6935 - accuracy: 0.7438 - val_loss: 0.8278 - val_accuracy: 0.7014\n",
      "Epoch 273/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6930 - accuracy: 0.7497 - val_loss: 0.8326 - val_accuracy: 0.6967\n",
      "Epoch 274/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6924 - accuracy: 0.7453 - val_loss: 0.8234 - val_accuracy: 0.6943\n",
      "Epoch 275/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.6899 - accuracy: 0.7417 - val_loss: 0.8215 - val_accuracy: 0.6967\n",
      "Epoch 276/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6886 - accuracy: 0.7485 - val_loss: 0.8194 - val_accuracy: 0.7062\n",
      "Epoch 277/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.6890 - accuracy: 0.7456 - val_loss: 0.8265 - val_accuracy: 0.7050\n",
      "Epoch 278/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.6874 - accuracy: 0.7497 - val_loss: 0.8151 - val_accuracy: 0.7085\n",
      "Epoch 279/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6888 - accuracy: 0.7444 - val_loss: 0.8206 - val_accuracy: 0.7097\n",
      "Epoch 280/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6853 - accuracy: 0.7491 - val_loss: 0.8167 - val_accuracy: 0.7062\n",
      "Epoch 281/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6860 - accuracy: 0.7500 - val_loss: 0.8167 - val_accuracy: 0.7038\n",
      "Epoch 282/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.6841 - accuracy: 0.7479 - val_loss: 0.8196 - val_accuracy: 0.7062\n",
      "Epoch 283/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6864 - accuracy: 0.7494 - val_loss: 0.8162 - val_accuracy: 0.6931\n",
      "Epoch 284/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6850 - accuracy: 0.7429 - val_loss: 0.8165 - val_accuracy: 0.7038\n",
      "Epoch 285/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.6821 - accuracy: 0.7506 - val_loss: 0.8246 - val_accuracy: 0.6967\n",
      "Epoch 286/300\n",
      "3372/3372 [==============================] - 0s 28us/step - loss: 0.6814 - accuracy: 0.7473 - val_loss: 0.8193 - val_accuracy: 0.6943\n",
      "Epoch 287/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.6834 - accuracy: 0.7488 - val_loss: 0.8106 - val_accuracy: 0.7050\n",
      "Epoch 288/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.6814 - accuracy: 0.7509 - val_loss: 0.8134 - val_accuracy: 0.7026\n",
      "Epoch 289/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.6809 - accuracy: 0.7473 - val_loss: 0.8203 - val_accuracy: 0.7062\n",
      "Epoch 290/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.6803 - accuracy: 0.7521 - val_loss: 0.8114 - val_accuracy: 0.7038\n",
      "Epoch 291/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6798 - accuracy: 0.7497 - val_loss: 0.8167 - val_accuracy: 0.7038\n",
      "Epoch 292/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6790 - accuracy: 0.7530 - val_loss: 0.8072 - val_accuracy: 0.7073\n",
      "Epoch 293/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6775 - accuracy: 0.7506 - val_loss: 0.8054 - val_accuracy: 0.7002\n",
      "Epoch 294/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6771 - accuracy: 0.7464 - val_loss: 0.8111 - val_accuracy: 0.7002\n",
      "Epoch 295/300\n",
      "3372/3372 [==============================] - 0s 26us/step - loss: 0.6749 - accuracy: 0.7497 - val_loss: 0.8041 - val_accuracy: 0.7038\n",
      "Epoch 296/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.6738 - accuracy: 0.7503 - val_loss: 0.8064 - val_accuracy: 0.7097\n",
      "Epoch 297/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.6727 - accuracy: 0.7488 - val_loss: 0.8075 - val_accuracy: 0.7121\n",
      "Epoch 298/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.6727 - accuracy: 0.7488 - val_loss: 0.8093 - val_accuracy: 0.7062\n",
      "Epoch 299/300\n",
      "3372/3372 [==============================] - 0s 25us/step - loss: 0.6741 - accuracy: 0.7494 - val_loss: 0.8042 - val_accuracy: 0.7121\n",
      "Epoch 300/300\n",
      "3372/3372 [==============================] - 0s 27us/step - loss: 0.6704 - accuracy: 0.7544 - val_loss: 0.8101 - val_accuracy: 0.7050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7efb9f646978>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "y_val = keras.utils.to_categorical(y_val)\n",
    "\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Dense(units = 11, kernel_initializer = 'uniform', activation = 'tanh', input_dim = 10))\n",
    "\n",
    "\n",
    "classifier.add(Dense(units= 11, kernel_initializer = 'uniform', activation = 'tanh'))\n",
    "\n",
    "classifier.add(Dense(units= 11, kernel_initializer = 'uniform', activation = 'tanh'))\n",
    "\n",
    "classifier.add(Dense(units= 12, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=8, mode='auto') \n",
    "classifier.fit(X_train, y_train, nb_epoch = 300, validation_data=(X_val, y_val),callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844/844 [==============================] - 0s 8us/step\n",
      "accuracy: 70.50%\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "scores = classifier.evaluate(X_test, y_test)\n",
    "print(\"%s: %.2f%%\" % (classifier.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test.argmax(axis = 1), y_pred.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I believe the low accuracy is due to the fact that the technique of generating new instances cannot provide data to mimic the real world data in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
